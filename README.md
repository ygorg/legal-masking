# legal-masking

Instead of randomly masking words for the MLM (maked language modeling) objective in BERT-like models, we mask "important" words for the domain (terms), document (keywords), genre (meta-discourse).