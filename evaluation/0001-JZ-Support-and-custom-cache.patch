From 4f46d6ac99e48f71e9426fcb1d87176ce14958fd Mon Sep 17 00:00:00 2001
From: ygorg <ygor.gallina@univ-nantes.fr>
Date: Fri, 1 Mar 2024 15:38:57 +0100
Subject: [PATCH] JZ Support and custom cache

---
 experiments/case_hold.py        |  53 +++-
 experiments/casehold_helpers.py |  10 +-
 experiments/ecthr.py            |   6 +
 experiments/ecthr_a.py          | 524 ++++++++++++++++++++++++++++++++
 experiments/ecthr_b.py          | 524 ++++++++++++++++++++++++++++++++
 experiments/eurlex.py           |  34 ++-
 experiments/ledgar.py           |  36 ++-
 experiments/scotus.py           |  42 ++-
 experiments/unfair_tos.py       |  41 ++-
 9 files changed, 1237 insertions(+), 33 deletions(-)
 create mode 100644 experiments/ecthr_a.py
 create mode 100644 experiments/ecthr_b.py

diff --git a/experiments/case_hold.py b/experiments/case_hold.py
index 085ec48..009f8d4 100644
--- a/experiments/case_hold.py
+++ b/experiments/case_hold.py
@@ -4,11 +4,19 @@
 
 import logging
 import os
+import random
+
+
 from dataclasses import dataclass, field
 from typing import Optional
 
+
 import numpy as np
-import random
+
+from sklearn.metrics import f1_score
+
+
+
 import shutil
 import glob
 
@@ -17,18 +25,25 @@ from transformers import (
 	AutoConfig,
 	AutoModelForMultipleChoice,
 	AutoTokenizer,
+
 	EvalPrediction,
 	HfArgumentParser,
-	Trainer,
 	TrainingArguments,
+
 	set_seed,
+	EarlyStoppingCallback,
+	Trainer,
 )
 from transformers.trainer_utils import is_main_process
-from transformers import EarlyStoppingCallback
 from casehold_helpers import MultipleChoiceDataset, Split
-from sklearn.metrics import f1_score
 from models.deberta import DebertaForMultipleChoice
-
+from transformers import BertForMultipleChoice, BertConfig
+try:
+	import idr_torch
+	JZ = True
+except ModuleNotFoundError:
+	JZ = False
+import torch.distributed as dist
 
 logger = logging.getLogger(__name__)
 
@@ -99,6 +114,12 @@ class DataTrainingArguments:
 	overwrite_cache: bool = field(
 		default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
 	)
+	task: Optional[str] = field(
+		default='ecthr_a',
+		metadata={
+			"help": "Define downstream task"
+		},
+	)
 
 
 def main():
@@ -106,11 +127,23 @@ def main():
 	# or by passing the --help flag to this script.
 	# We now keep distinct sets of args, for a cleaner separation of concerns.
 
+	if JZ:
+		dist.init_process_group(
+			backend='nccl',
+			init_method='env://',
+			world_size=idr_torch.size,
+			rank=idr_torch.rank
+		)
+
 	parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
 	# Add custom arguments for computing pre-train loss
 	parser.add_argument("--ptl", type=bool, default=False)
 	model_args, data_args, training_args, custom_args = parser.parse_args_into_dataclasses()
 
+	if JZ:
+		training_args.local_rank = idr_torch.local_rank
+
+
 	if (
 		os.path.exists(training_args.output_dir)
 		and os.listdir(training_args.output_dir)
@@ -160,12 +193,13 @@ def main():
 
 	tokenizer = AutoTokenizer.from_pretrained(
 		model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
-		cache_dir=model_args.cache_dir,
 		# Default fast tokenizer is buggy on CaseHOLD task, switch to legacy tokenizer
-		use_fast=True,
+		use_fast=True
 	)
+	if config.model_type == 'bert':
+		model = AutoModelForMultipleChoice.from_pretrained(model_args.model_name_or_path)
 
-	if config.model_type != 'deberta':
+	elif config.model_type != 'deberta':
 		model = AutoModelForMultipleChoice.from_pretrained(
 			model_args.model_name_or_path,
 			from_tf=bool(".ckpt" in model_args.model_name_or_path),
@@ -188,6 +222,7 @@ def main():
 		train_dataset = \
 			MultipleChoiceDataset(
 				tokenizer=tokenizer,
+				cache_dir= model_args.cache_dir,
 				task=data_args.task_name,
 				max_seq_length=data_args.max_seq_length,
 				overwrite_cache=data_args.overwrite_cache,
@@ -199,6 +234,7 @@ def main():
 		eval_dataset = \
 			MultipleChoiceDataset(
 				tokenizer=tokenizer,
+				cache_dir= model_args.cache_dir,
 				task=data_args.task_name,
 				max_seq_length=data_args.max_seq_length,
 				overwrite_cache=data_args.overwrite_cache,
@@ -209,6 +245,7 @@ def main():
 		predict_dataset = \
 			MultipleChoiceDataset(
 				tokenizer=tokenizer,
+				cache_dir= model_args.cache_dir,
 				task=data_args.task_name,
 				max_seq_length=data_args.max_seq_length,
 				overwrite_cache=data_args.overwrite_cache,
diff --git a/experiments/casehold_helpers.py b/experiments/casehold_helpers.py
index 0db4adf..d351f26 100644
--- a/experiments/casehold_helpers.py
+++ b/experiments/casehold_helpers.py
@@ -47,12 +47,18 @@ if is_torch_available():
         def __init__(
             self,
             tokenizer: PreTrainedTokenizer,
+            cache_dir: str,
             task: str,
             max_seq_length: Optional[int] = None,
             overwrite_cache=False,
             mode: Split = Split.train,
         ):
-            dataset = datasets.load_dataset('lex_glue', task)
+            #dataset = datasets.load_dataset('lex_glue', task)
+            dataset={}
+            dataset["train"] = datasets.load_from_disk(f"{cache_dir}/{task}/train")
+            dataset["validation"] = datasets.load_from_disk(f"{cache_dir}/{task}/validation")
+            dataset["test"] = datasets.load_from_disk(f"{cache_dir}/{task}/test")
+            # dataset = datasets.load_from_disk(f"{cache_dir}/{task}")
             tokenizer_name = re.sub('[^a-z]+', ' ', tokenizer.name_or_path).title().replace(' ', '')
             cached_features_file = os.path.join(
                 '.cache',
@@ -119,7 +125,7 @@ if is_tf_available():
             overwrite_cache=False,
             mode: Split = Split.train,
         ):
-            dataset = datasets.load_dataset('lex_glue')
+            dataset = datasets.load_from_disk(f"{cache_dir}/{task}")
 
             logger.info(f"Creating features from dataset file at {task}")
             if mode == Split.dev:
diff --git a/experiments/ecthr.py b/experiments/ecthr.py
index 0d1ec25..ab22452 100644
--- a/experiments/ecthr.py
+++ b/experiments/ecthr.py
@@ -5,6 +5,7 @@
 import logging
 import os
 import random
+
 import sys
 from dataclasses import dataclass, field
 from typing import Optional
@@ -32,6 +33,7 @@ from transformers import (
     set_seed,
     EarlyStoppingCallback,
 )
+
 from transformers.trainer_utils import get_last_checkpoint
 from transformers.utils import check_min_version
 from transformers.utils.versions import require_version
@@ -39,6 +41,10 @@ from models.hierbert import HierarchicalBert
 from models.deberta import DebertaForSequenceClassification
 
 
+
+
+
+
 # Will error if the minimal version of Transformers is not installed. Remove at your own risks.
 check_min_version("4.9.0")
 
diff --git a/experiments/ecthr_a.py b/experiments/ecthr_a.py
new file mode 100644
index 0000000..6ab1373
--- /dev/null
+++ b/experiments/ecthr_a.py
@@ -0,0 +1,524 @@
+#!/usr/bin/env python
+# coding=utf-8
+""" Finetuning models on the ECtHR dataset (e.g. Bert, RoBERTa, LEGAL-BERT)."""
+
+import logging
+import os
+import random
+
+import sys
+from dataclasses import dataclass, field
+from typing import Optional
+
+import datasets
+import numpy as np
+from datasets import load_dataset
+from sklearn.metrics import f1_score
+from trainer import MultilabelTrainer
+from scipy.special import expit
+from torch import nn
+import glob
+import shutil
+
+import transformers
+from transformers import (
+    AutoConfig,
+    AutoModelForSequenceClassification,
+    AutoTokenizer,
+    DataCollatorWithPadding,
+    EvalPrediction,
+    HfArgumentParser,
+    TrainingArguments,
+    default_data_collator,
+    set_seed,
+    EarlyStoppingCallback,
+)
+
+from transformers.trainer_utils import get_last_checkpoint
+from transformers.utils import check_min_version
+from transformers.utils.versions import require_version
+try:
+    import idr_torch
+    JZ = True
+except ModuleNotFoundError:
+    JZ = False
+import torch.distributed as dist
+
+# Will error if the minimal version of Transformers is not installed. Remove at your own risks.
+check_min_version("4.9.0")
+
+require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/text-classification/requirements.txt")
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class DataTrainingArguments:
+    """
+    Arguments pertaining to what data we are going to input our model for training and eval.
+
+    Using `HfArgumentParser` we can turn this class
+    into argparse arguments to be able to specify them on
+    the command line.
+    """
+
+    max_seq_length: Optional[int] = field(
+        default=4096,
+        metadata={
+            "help": "The maximum total input sequence length after tokenization. Sequences longer "
+            "than this will be truncated, sequences shorter will be padded."
+        },
+    )
+    max_segments: Optional[int] = field(
+        default=64,
+        metadata={
+            "help": "The maximum number of segments (paragraphs) to be considered. Sequences longer "
+                    "than this will be truncated, sequences shorter will be padded."
+        },
+    )
+    max_seg_length: Optional[int] = field(
+        default=128,
+        metadata={
+            "help": "The maximum segment (paragraph) length to be considered. Segments longer "
+                    "than this will be truncated, sequences shorter will be padded."
+        },
+    )
+    overwrite_cache: bool = field(
+        default=False, metadata={"help": "Overwrite the cached preprocessed datasets or not."}
+    )
+    pad_to_max_length: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to pad all samples to `max_seq_length`. "
+            "If False, will pad the samples dynamically when batching to the maximum length in the batch."
+        },
+    )
+    max_train_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of training examples to this "
+            "value if set."
+        },
+    )
+    max_eval_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
+            "value if set."
+        },
+    )
+    max_predict_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of prediction examples to this "
+            "value if set."
+        },
+    )
+    task: Optional[str] = field(
+        default='ecthr_a',
+        metadata={
+            "help": "Define downstream task"
+        },
+    )
+    server_ip: Optional[str] = field(default=None, metadata={"help": "For distant debugging."})
+    server_port: Optional[str] = field(default=None, metadata={"help": "For distant debugging."})
+
+
+@dataclass
+class ModelArguments:
+    """
+    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
+    """
+
+    model_name_or_path: str = field(
+        default=None, metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
+    )
+    hierarchical: bool = field(
+        default=True, metadata={"help": "Whether to use a hierarchical variant or not"}
+    )
+    config_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
+    )
+    tokenizer_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
+    )
+    cache_dir: Optional[str] = field(
+        default=None,
+        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
+    )
+    do_lower_case: Optional[bool] = field(
+        default=True,
+        metadata={"help": "arg to indicate if tokenizer should do lower case in AutoTokenizer.from_pretrained()"},
+    )
+    use_fast_tokenizer: bool = field(
+        default=True,
+        metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
+    )
+    model_revision: str = field(
+        default="main",
+        metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
+    )
+    use_auth_token: bool = field(
+        default=False,
+        metadata={
+            "help": "Will use the token generated when running `transformers-cli login` (necessary to use this script "
+            "with private models)."
+        },
+    )
+
+
+def main():
+    # See all possible arguments in src/transformers/training_args.py
+    # or by passing the --help flag to this script.
+    # We now keep distinct sets of args, for a cleaner separation of concerns.
+
+    if JZ:
+        dist.init_process_group(
+            backend='nccl',
+            init_method='env://',
+            world_size=idr_torch.size,
+            rank=idr_torch.rank
+        )
+
+    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
+    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
+
+    if JZ:
+        training_args.local_rank = idr_torch.local_rank
+
+    # Fix boolean parameter
+    if model_args.do_lower_case == 'False' or not model_args.do_lower_case:
+        model_args.do_lower_case = False
+    else:
+        model_args.do_lower_case = True
+
+    if model_args.hierarchical == 'False' or not model_args.hierarchical:
+        model_args.hierarchical = False
+    else:
+        model_args.hierarchical = True
+
+    # Setup distant debugging if needed
+    if data_args.server_ip and data_args.server_port:
+        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script
+        import ptvsd
+
+        print("Waiting for debugger attach")
+        ptvsd.enable_attach(address=(data_args.server_ip, data_args.server_port), redirect_output=True)
+        ptvsd.wait_for_attach()
+
+    # Setup logging
+    logging.basicConfig(
+        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
+        datefmt="%m/%d/%Y %H:%M:%S",
+        handlers=[logging.StreamHandler(sys.stdout)],
+    )
+
+    log_level = training_args.get_process_log_level()
+    logger.setLevel(log_level)
+    datasets.utils.logging.set_verbosity(log_level)
+    transformers.utils.logging.set_verbosity(log_level)
+    transformers.utils.logging.enable_default_handler()
+    transformers.utils.logging.enable_explicit_format()
+
+    # Log on each process the small summary:
+    logger.warning(
+        f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
+        + f"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}"
+    )
+    logger.info(f"Training/evaluation parameters {training_args}")
+
+    # Detecting last checkpoint.
+    last_checkpoint = None
+    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
+        last_checkpoint = get_last_checkpoint(training_args.output_dir)
+        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
+            raise ValueError(
+                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
+                "Use --overwrite_output_dir to overcome."
+            )
+        elif last_checkpoint is not None:
+            logger.info(
+                f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
+                "the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
+            )
+
+    # Set seed before initializing model.
+    set_seed(training_args.seed)
+
+    # In distributed training, the load_dataset function guarantees that only one local process can concurrently
+    # download the dataset.
+    # Downloading and loading eurlex dataset from the hub.
+
+
+
+    if training_args.do_train:
+        train_dataset = datasets.load_from_disk(f"{model_args.cache_dir}/{data_args.task}/train")
+
+    if training_args.do_eval:
+        eval_dataset = datasets.load_from_disk(f"{model_args.cache_dir}/{data_args.task}/validation")
+
+    if training_args.do_predict:
+        predict_dataset = datasets.load_from_disk(f"{model_args.cache_dir}/{data_args.task}/test")
+
+    # Labels
+    label_list = list(range(10))
+    num_labels = len(label_list)
+
+    # Load pretrained model and tokenizer
+    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently
+    # download model & vocab.
+    config = AutoConfig.from_pretrained(
+        model_args.config_name if model_args.config_name else model_args.model_name_or_path,
+        num_labels=num_labels,
+        finetuning_task=f"{data_args.task}",
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+    tokenizer = AutoTokenizer.from_pretrained(
+        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
+        do_lower_case=model_args.do_lower_case,
+        cache_dir=model_args.cache_dir,
+        use_fast=model_args.use_fast_tokenizer,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+    if config.model_type == 'deberta' and model_args.hierarchical:
+        model = DebertaForSequenceClassification.from_pretrained(
+            model_args.model_name_or_path,
+            from_tf=bool(".ckpt" in model_args.model_name_or_path),
+            config=config,
+            cache_dir=model_args.cache_dir,
+            revision=model_args.model_revision,
+            use_auth_token=True if model_args.use_auth_token else None,
+        )
+    else:
+        model = AutoModelForSequenceClassification.from_pretrained(
+            model_args.model_name_or_path,
+            from_tf=bool(".ckpt" in model_args.model_name_or_path),
+            config=config,
+            cache_dir=model_args.cache_dir,
+            revision=model_args.model_revision,
+            use_auth_token=True if model_args.use_auth_token else None,
+        )
+
+    if model_args.hierarchical:
+        # Hack the classifier encoder to use hierarchical BERT
+        if config.model_type in ['bert', 'deberta']:
+            if config.model_type == 'bert':
+                segment_encoder = model.bert
+            else:
+                segment_encoder = model.deberta
+            model_encoder = HierarchicalBert(encoder=segment_encoder,
+                                             max_segments=data_args.max_segments,
+                                             max_segment_length=data_args.max_seg_length)
+            if config.model_type == 'bert':
+                model.bert = model_encoder
+            elif config.model_type == 'deberta':
+                model.deberta = model_encoder
+            else:
+                raise NotImplementedError(f"{config.model_type} is no supported yet!")
+        elif config.model_type == 'roberta':
+            model_encoder = HierarchicalBert(encoder=model.roberta, max_segments=data_args.max_segments,
+                                             max_segment_length=data_args.max_seg_length)
+            model.roberta = model_encoder
+            # Build a new classification layer, as well
+            dense = nn.Linear(config.hidden_size, config.hidden_size)
+            dense.load_state_dict(model.classifier.dense.state_dict())  # load weights
+            dropout = nn.Dropout(config.hidden_dropout_prob).to(model.device)
+            out_proj = nn.Linear(config.hidden_size, config.num_labels).to(model.device)
+            out_proj.load_state_dict(model.classifier.out_proj.state_dict())  # load weights
+            model.classifier = nn.Sequential(dense, dropout, out_proj).to(model.device)
+        elif config.model_type in ['longformer', 'big_bird']:
+            pass
+        else:
+            raise NotImplementedError(f"{config.model_type} is no supported yet!")
+
+    # Preprocessing the datasets
+    # Padding strategy
+    if data_args.pad_to_max_length:
+        padding = "max_length"
+    else:
+        # We will pad later, dynamically at batch creation, to the max sequence length in each batch
+        padding = False
+
+    def preprocess_function(examples):
+        # Tokenize the texts
+        if model_args.hierarchical:
+            case_template = [[0] * data_args.max_seg_length]
+            if config.model_type == 'roberta':
+                batch = {'input_ids': [], 'attention_mask': []}
+                for case in examples['text']:
+                    case_encodings = tokenizer(case[:data_args.max_segments], padding=padding,
+                                               max_length=data_args.max_seg_length, truncation=True)
+                    batch['input_ids'].append(case_encodings['input_ids'] + case_template * (
+                                data_args.max_segments - len(case_encodings['input_ids'])))
+                    batch['attention_mask'].append(case_encodings['attention_mask'] + case_template * (
+                                data_args.max_segments - len(case_encodings['attention_mask'])))
+            else:
+                batch = {'input_ids': [], 'attention_mask': [], 'token_type_ids': []}
+                for case in examples['text']:
+                    case_encodings = tokenizer(case[:data_args.max_segments], padding=padding,
+                                               max_length=data_args.max_seg_length, truncation=True)
+                    batch['input_ids'].append(case_encodings['input_ids'] + case_template * (
+                            data_args.max_segments - len(case_encodings['input_ids'])))
+                    batch['attention_mask'].append(case_encodings['attention_mask'] + case_template * (
+                            data_args.max_segments - len(case_encodings['attention_mask'])))
+                    batch['token_type_ids'].append(case_encodings['token_type_ids'] + case_template * (
+                            data_args.max_segments - len(case_encodings['token_type_ids'])))
+        elif config.model_type in ['longformer', 'big_bird']:
+            cases = []
+            max_position_embeddings = config.max_position_embeddings - 2 if config.model_type == 'longformer' \
+                else config.max_position_embeddings
+            for case in examples['text']:
+                cases.append(f' {tokenizer.sep_token} '.join(
+                    [' '.join(fact.split()[:data_args.max_seg_length]) for fact in case[:data_args.max_segments]]))
+            batch = tokenizer(cases, padding=padding, max_length=max_position_embeddings, truncation=True)
+            if config.model_type == 'longformer':
+                global_attention_mask = np.zeros((len(cases), max_position_embeddings), dtype=np.int32)
+                # global attention on cls token
+                global_attention_mask[:, 0] = 1
+                batch['global_attention_mask'] = list(global_attention_mask)
+        else:
+            cases = []
+            for case in examples['text']:
+                cases.append(f'\n'.join(case))
+            batch = tokenizer(cases, padding=padding, max_length=512, truncation=True)
+
+        batch["labels"] = [[1 if label in labels else 0 for label in label_list] for labels in examples["labels"]]
+
+        return batch
+
+    if training_args.do_train:
+        if data_args.max_train_samples is not None:
+            train_dataset = train_dataset.select(range(data_args.max_train_samples))
+        with training_args.main_process_first(desc="train dataset map pre-processing"):
+            train_dataset = train_dataset.map(
+                preprocess_function,
+                batched=True,
+                load_from_cache_file=not data_args.overwrite_cache,
+                desc="Running tokenizer on train dataset",
+            )
+        # Log a few random samples from the training set:
+        for index in random.sample(range(len(train_dataset)), 3):
+            logger.info(f"Sample {index} of the training set: {train_dataset[index]}.")
+
+    if training_args.do_eval:
+        if data_args.max_eval_samples is not None:
+            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))
+        with training_args.main_process_first(desc="validation dataset map pre-processing"):
+            eval_dataset = eval_dataset.map(
+                preprocess_function,
+                batched=True,
+                load_from_cache_file=not data_args.overwrite_cache,
+                desc="Running tokenizer on validation dataset",
+            )
+
+    if training_args.do_predict:
+        if data_args.max_predict_samples is not None:
+            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))
+        with training_args.main_process_first(desc="prediction dataset map pre-processing"):
+            predict_dataset = predict_dataset.map(
+                preprocess_function,
+                batched=True,
+                load_from_cache_file=not data_args.overwrite_cache,
+                desc="Running tokenizer on prediction dataset",
+            )
+
+    # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a
+    # predictions and label_ids field) and has to return a dictionary string to float.
+    def compute_metrics(p: EvalPrediction):
+        # Fix gold labels
+        y_true = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)
+        y_true[:, :-1] = p.label_ids
+        y_true[:, -1] = (np.sum(p.label_ids, axis=1) == 0).astype('int32')
+        # Fix predictions
+        logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
+        preds = (expit(logits) > 0.5).astype('int32')
+        y_pred = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)
+        y_pred[:, :-1] = preds
+        y_pred[:, -1] = (np.sum(preds, axis=1) == 0).astype('int32')
+        # Compute scores
+        macro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0)
+        micro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)
+        return {'macro-f1': macro_f1, 'micro-f1': micro_f1}
+
+    # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.
+    if data_args.pad_to_max_length:
+        data_collator = default_data_collator
+    elif training_args.fp16:
+        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)
+    else:
+        data_collator = None
+
+    # Initialize our Trainer
+    trainer = MultilabelTrainer(
+        model=model,
+        args=training_args,
+        train_dataset=train_dataset if training_args.do_train else None,
+        eval_dataset=eval_dataset if training_args.do_eval else None,
+        compute_metrics=compute_metrics,
+        tokenizer=tokenizer,
+        data_collator=data_collator,
+        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
+    )
+
+    # Training
+    if training_args.do_train:
+        checkpoint = None
+        if training_args.resume_from_checkpoint is not None:
+            checkpoint = training_args.resume_from_checkpoint
+        elif last_checkpoint is not None:
+            checkpoint = last_checkpoint
+        train_result = trainer.train(resume_from_checkpoint=checkpoint)
+        metrics = train_result.metrics
+        max_train_samples = (
+            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
+        )
+        metrics["train_samples"] = min(max_train_samples, len(train_dataset))
+
+        trainer.save_model()  # Saves the tokenizer too for easy upload
+
+        trainer.log_metrics("train", metrics)
+        trainer.save_metrics("train", metrics)
+        trainer.save_state()
+
+    # Evaluation
+    if training_args.do_eval:
+        logger.info("*** Evaluate ***")
+        metrics = trainer.evaluate(eval_dataset=eval_dataset)
+
+        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
+        metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))
+
+        trainer.log_metrics("eval", metrics)
+        trainer.save_metrics("eval", metrics)
+
+    # Prediction
+    if training_args.do_predict:
+        logger.info("*** Predict ***")
+        predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix="predict")
+
+        max_predict_samples = (
+            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)
+        )
+        metrics["predict_samples"] = min(max_predict_samples, len(predict_dataset))
+
+        trainer.log_metrics("predict", metrics)
+        trainer.save_metrics("predict", metrics)
+
+        output_predict_file = os.path.join(training_args.output_dir, "test_predictions.csv")
+        if trainer.is_world_process_zero():
+            with open(output_predict_file, "w") as writer:
+                for index, pred_list in enumerate(predictions[0]):
+                    pred_line = '\t'.join([f'{pred:.5f}' for pred in pred_list])
+                    writer.write(f"{index}\t{pred_line}\n")
+
+    # Clean up checkpoints
+    checkpoints = [filepath for filepath in glob.glob(f'{training_args.output_dir}/*/') if '/checkpoint' in filepath]
+    for checkpoint in checkpoints:
+        shutil.rmtree(checkpoint)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/experiments/ecthr_b.py b/experiments/ecthr_b.py
new file mode 100644
index 0000000..6ab1373
--- /dev/null
+++ b/experiments/ecthr_b.py
@@ -0,0 +1,524 @@
+#!/usr/bin/env python
+# coding=utf-8
+""" Finetuning models on the ECtHR dataset (e.g. Bert, RoBERTa, LEGAL-BERT)."""
+
+import logging
+import os
+import random
+
+import sys
+from dataclasses import dataclass, field
+from typing import Optional
+
+import datasets
+import numpy as np
+from datasets import load_dataset
+from sklearn.metrics import f1_score
+from trainer import MultilabelTrainer
+from scipy.special import expit
+from torch import nn
+import glob
+import shutil
+
+import transformers
+from transformers import (
+    AutoConfig,
+    AutoModelForSequenceClassification,
+    AutoTokenizer,
+    DataCollatorWithPadding,
+    EvalPrediction,
+    HfArgumentParser,
+    TrainingArguments,
+    default_data_collator,
+    set_seed,
+    EarlyStoppingCallback,
+)
+
+from transformers.trainer_utils import get_last_checkpoint
+from transformers.utils import check_min_version
+from transformers.utils.versions import require_version
+try:
+    import idr_torch
+    JZ = True
+except ModuleNotFoundError:
+    JZ = False
+import torch.distributed as dist
+
+# Will error if the minimal version of Transformers is not installed. Remove at your own risks.
+check_min_version("4.9.0")
+
+require_version("datasets>=1.8.0", "To fix: pip install -r examples/pytorch/text-classification/requirements.txt")
+
+logger = logging.getLogger(__name__)
+
+
+@dataclass
+class DataTrainingArguments:
+    """
+    Arguments pertaining to what data we are going to input our model for training and eval.
+
+    Using `HfArgumentParser` we can turn this class
+    into argparse arguments to be able to specify them on
+    the command line.
+    """
+
+    max_seq_length: Optional[int] = field(
+        default=4096,
+        metadata={
+            "help": "The maximum total input sequence length after tokenization. Sequences longer "
+            "than this will be truncated, sequences shorter will be padded."
+        },
+    )
+    max_segments: Optional[int] = field(
+        default=64,
+        metadata={
+            "help": "The maximum number of segments (paragraphs) to be considered. Sequences longer "
+                    "than this will be truncated, sequences shorter will be padded."
+        },
+    )
+    max_seg_length: Optional[int] = field(
+        default=128,
+        metadata={
+            "help": "The maximum segment (paragraph) length to be considered. Segments longer "
+                    "than this will be truncated, sequences shorter will be padded."
+        },
+    )
+    overwrite_cache: bool = field(
+        default=False, metadata={"help": "Overwrite the cached preprocessed datasets or not."}
+    )
+    pad_to_max_length: bool = field(
+        default=True,
+        metadata={
+            "help": "Whether to pad all samples to `max_seq_length`. "
+            "If False, will pad the samples dynamically when batching to the maximum length in the batch."
+        },
+    )
+    max_train_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of training examples to this "
+            "value if set."
+        },
+    )
+    max_eval_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of evaluation examples to this "
+            "value if set."
+        },
+    )
+    max_predict_samples: Optional[int] = field(
+        default=None,
+        metadata={
+            "help": "For debugging purposes or quicker training, truncate the number of prediction examples to this "
+            "value if set."
+        },
+    )
+    task: Optional[str] = field(
+        default='ecthr_a',
+        metadata={
+            "help": "Define downstream task"
+        },
+    )
+    server_ip: Optional[str] = field(default=None, metadata={"help": "For distant debugging."})
+    server_port: Optional[str] = field(default=None, metadata={"help": "For distant debugging."})
+
+
+@dataclass
+class ModelArguments:
+    """
+    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
+    """
+
+    model_name_or_path: str = field(
+        default=None, metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
+    )
+    hierarchical: bool = field(
+        default=True, metadata={"help": "Whether to use a hierarchical variant or not"}
+    )
+    config_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
+    )
+    tokenizer_name: Optional[str] = field(
+        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
+    )
+    cache_dir: Optional[str] = field(
+        default=None,
+        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
+    )
+    do_lower_case: Optional[bool] = field(
+        default=True,
+        metadata={"help": "arg to indicate if tokenizer should do lower case in AutoTokenizer.from_pretrained()"},
+    )
+    use_fast_tokenizer: bool = field(
+        default=True,
+        metadata={"help": "Whether to use one of the fast tokenizer (backed by the tokenizers library) or not."},
+    )
+    model_revision: str = field(
+        default="main",
+        metadata={"help": "The specific model version to use (can be a branch name, tag name or commit id)."},
+    )
+    use_auth_token: bool = field(
+        default=False,
+        metadata={
+            "help": "Will use the token generated when running `transformers-cli login` (necessary to use this script "
+            "with private models)."
+        },
+    )
+
+
+def main():
+    # See all possible arguments in src/transformers/training_args.py
+    # or by passing the --help flag to this script.
+    # We now keep distinct sets of args, for a cleaner separation of concerns.
+
+    if JZ:
+        dist.init_process_group(
+            backend='nccl',
+            init_method='env://',
+            world_size=idr_torch.size,
+            rank=idr_torch.rank
+        )
+
+    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
+    model_args, data_args, training_args = parser.parse_args_into_dataclasses()
+
+    if JZ:
+        training_args.local_rank = idr_torch.local_rank
+
+    # Fix boolean parameter
+    if model_args.do_lower_case == 'False' or not model_args.do_lower_case:
+        model_args.do_lower_case = False
+    else:
+        model_args.do_lower_case = True
+
+    if model_args.hierarchical == 'False' or not model_args.hierarchical:
+        model_args.hierarchical = False
+    else:
+        model_args.hierarchical = True
+
+    # Setup distant debugging if needed
+    if data_args.server_ip and data_args.server_port:
+        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script
+        import ptvsd
+
+        print("Waiting for debugger attach")
+        ptvsd.enable_attach(address=(data_args.server_ip, data_args.server_port), redirect_output=True)
+        ptvsd.wait_for_attach()
+
+    # Setup logging
+    logging.basicConfig(
+        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
+        datefmt="%m/%d/%Y %H:%M:%S",
+        handlers=[logging.StreamHandler(sys.stdout)],
+    )
+
+    log_level = training_args.get_process_log_level()
+    logger.setLevel(log_level)
+    datasets.utils.logging.set_verbosity(log_level)
+    transformers.utils.logging.set_verbosity(log_level)
+    transformers.utils.logging.enable_default_handler()
+    transformers.utils.logging.enable_explicit_format()
+
+    # Log on each process the small summary:
+    logger.warning(
+        f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
+        + f"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}"
+    )
+    logger.info(f"Training/evaluation parameters {training_args}")
+
+    # Detecting last checkpoint.
+    last_checkpoint = None
+    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:
+        last_checkpoint = get_last_checkpoint(training_args.output_dir)
+        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:
+            raise ValueError(
+                f"Output directory ({training_args.output_dir}) already exists and is not empty. "
+                "Use --overwrite_output_dir to overcome."
+            )
+        elif last_checkpoint is not None:
+            logger.info(
+                f"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change "
+                "the `--output_dir` or add `--overwrite_output_dir` to train from scratch."
+            )
+
+    # Set seed before initializing model.
+    set_seed(training_args.seed)
+
+    # In distributed training, the load_dataset function guarantees that only one local process can concurrently
+    # download the dataset.
+    # Downloading and loading eurlex dataset from the hub.
+
+
+
+    if training_args.do_train:
+        train_dataset = datasets.load_from_disk(f"{model_args.cache_dir}/{data_args.task}/train")
+
+    if training_args.do_eval:
+        eval_dataset = datasets.load_from_disk(f"{model_args.cache_dir}/{data_args.task}/validation")
+
+    if training_args.do_predict:
+        predict_dataset = datasets.load_from_disk(f"{model_args.cache_dir}/{data_args.task}/test")
+
+    # Labels
+    label_list = list(range(10))
+    num_labels = len(label_list)
+
+    # Load pretrained model and tokenizer
+    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently
+    # download model & vocab.
+    config = AutoConfig.from_pretrained(
+        model_args.config_name if model_args.config_name else model_args.model_name_or_path,
+        num_labels=num_labels,
+        finetuning_task=f"{data_args.task}",
+        cache_dir=model_args.cache_dir,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+    tokenizer = AutoTokenizer.from_pretrained(
+        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,
+        do_lower_case=model_args.do_lower_case,
+        cache_dir=model_args.cache_dir,
+        use_fast=model_args.use_fast_tokenizer,
+        revision=model_args.model_revision,
+        use_auth_token=True if model_args.use_auth_token else None,
+    )
+    if config.model_type == 'deberta' and model_args.hierarchical:
+        model = DebertaForSequenceClassification.from_pretrained(
+            model_args.model_name_or_path,
+            from_tf=bool(".ckpt" in model_args.model_name_or_path),
+            config=config,
+            cache_dir=model_args.cache_dir,
+            revision=model_args.model_revision,
+            use_auth_token=True if model_args.use_auth_token else None,
+        )
+    else:
+        model = AutoModelForSequenceClassification.from_pretrained(
+            model_args.model_name_or_path,
+            from_tf=bool(".ckpt" in model_args.model_name_or_path),
+            config=config,
+            cache_dir=model_args.cache_dir,
+            revision=model_args.model_revision,
+            use_auth_token=True if model_args.use_auth_token else None,
+        )
+
+    if model_args.hierarchical:
+        # Hack the classifier encoder to use hierarchical BERT
+        if config.model_type in ['bert', 'deberta']:
+            if config.model_type == 'bert':
+                segment_encoder = model.bert
+            else:
+                segment_encoder = model.deberta
+            model_encoder = HierarchicalBert(encoder=segment_encoder,
+                                             max_segments=data_args.max_segments,
+                                             max_segment_length=data_args.max_seg_length)
+            if config.model_type == 'bert':
+                model.bert = model_encoder
+            elif config.model_type == 'deberta':
+                model.deberta = model_encoder
+            else:
+                raise NotImplementedError(f"{config.model_type} is no supported yet!")
+        elif config.model_type == 'roberta':
+            model_encoder = HierarchicalBert(encoder=model.roberta, max_segments=data_args.max_segments,
+                                             max_segment_length=data_args.max_seg_length)
+            model.roberta = model_encoder
+            # Build a new classification layer, as well
+            dense = nn.Linear(config.hidden_size, config.hidden_size)
+            dense.load_state_dict(model.classifier.dense.state_dict())  # load weights
+            dropout = nn.Dropout(config.hidden_dropout_prob).to(model.device)
+            out_proj = nn.Linear(config.hidden_size, config.num_labels).to(model.device)
+            out_proj.load_state_dict(model.classifier.out_proj.state_dict())  # load weights
+            model.classifier = nn.Sequential(dense, dropout, out_proj).to(model.device)
+        elif config.model_type in ['longformer', 'big_bird']:
+            pass
+        else:
+            raise NotImplementedError(f"{config.model_type} is no supported yet!")
+
+    # Preprocessing the datasets
+    # Padding strategy
+    if data_args.pad_to_max_length:
+        padding = "max_length"
+    else:
+        # We will pad later, dynamically at batch creation, to the max sequence length in each batch
+        padding = False
+
+    def preprocess_function(examples):
+        # Tokenize the texts
+        if model_args.hierarchical:
+            case_template = [[0] * data_args.max_seg_length]
+            if config.model_type == 'roberta':
+                batch = {'input_ids': [], 'attention_mask': []}
+                for case in examples['text']:
+                    case_encodings = tokenizer(case[:data_args.max_segments], padding=padding,
+                                               max_length=data_args.max_seg_length, truncation=True)
+                    batch['input_ids'].append(case_encodings['input_ids'] + case_template * (
+                                data_args.max_segments - len(case_encodings['input_ids'])))
+                    batch['attention_mask'].append(case_encodings['attention_mask'] + case_template * (
+                                data_args.max_segments - len(case_encodings['attention_mask'])))
+            else:
+                batch = {'input_ids': [], 'attention_mask': [], 'token_type_ids': []}
+                for case in examples['text']:
+                    case_encodings = tokenizer(case[:data_args.max_segments], padding=padding,
+                                               max_length=data_args.max_seg_length, truncation=True)
+                    batch['input_ids'].append(case_encodings['input_ids'] + case_template * (
+                            data_args.max_segments - len(case_encodings['input_ids'])))
+                    batch['attention_mask'].append(case_encodings['attention_mask'] + case_template * (
+                            data_args.max_segments - len(case_encodings['attention_mask'])))
+                    batch['token_type_ids'].append(case_encodings['token_type_ids'] + case_template * (
+                            data_args.max_segments - len(case_encodings['token_type_ids'])))
+        elif config.model_type in ['longformer', 'big_bird']:
+            cases = []
+            max_position_embeddings = config.max_position_embeddings - 2 if config.model_type == 'longformer' \
+                else config.max_position_embeddings
+            for case in examples['text']:
+                cases.append(f' {tokenizer.sep_token} '.join(
+                    [' '.join(fact.split()[:data_args.max_seg_length]) for fact in case[:data_args.max_segments]]))
+            batch = tokenizer(cases, padding=padding, max_length=max_position_embeddings, truncation=True)
+            if config.model_type == 'longformer':
+                global_attention_mask = np.zeros((len(cases), max_position_embeddings), dtype=np.int32)
+                # global attention on cls token
+                global_attention_mask[:, 0] = 1
+                batch['global_attention_mask'] = list(global_attention_mask)
+        else:
+            cases = []
+            for case in examples['text']:
+                cases.append(f'\n'.join(case))
+            batch = tokenizer(cases, padding=padding, max_length=512, truncation=True)
+
+        batch["labels"] = [[1 if label in labels else 0 for label in label_list] for labels in examples["labels"]]
+
+        return batch
+
+    if training_args.do_train:
+        if data_args.max_train_samples is not None:
+            train_dataset = train_dataset.select(range(data_args.max_train_samples))
+        with training_args.main_process_first(desc="train dataset map pre-processing"):
+            train_dataset = train_dataset.map(
+                preprocess_function,
+                batched=True,
+                load_from_cache_file=not data_args.overwrite_cache,
+                desc="Running tokenizer on train dataset",
+            )
+        # Log a few random samples from the training set:
+        for index in random.sample(range(len(train_dataset)), 3):
+            logger.info(f"Sample {index} of the training set: {train_dataset[index]}.")
+
+    if training_args.do_eval:
+        if data_args.max_eval_samples is not None:
+            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))
+        with training_args.main_process_first(desc="validation dataset map pre-processing"):
+            eval_dataset = eval_dataset.map(
+                preprocess_function,
+                batched=True,
+                load_from_cache_file=not data_args.overwrite_cache,
+                desc="Running tokenizer on validation dataset",
+            )
+
+    if training_args.do_predict:
+        if data_args.max_predict_samples is not None:
+            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))
+        with training_args.main_process_first(desc="prediction dataset map pre-processing"):
+            predict_dataset = predict_dataset.map(
+                preprocess_function,
+                batched=True,
+                load_from_cache_file=not data_args.overwrite_cache,
+                desc="Running tokenizer on prediction dataset",
+            )
+
+    # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a
+    # predictions and label_ids field) and has to return a dictionary string to float.
+    def compute_metrics(p: EvalPrediction):
+        # Fix gold labels
+        y_true = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)
+        y_true[:, :-1] = p.label_ids
+        y_true[:, -1] = (np.sum(p.label_ids, axis=1) == 0).astype('int32')
+        # Fix predictions
+        logits = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions
+        preds = (expit(logits) > 0.5).astype('int32')
+        y_pred = np.zeros((p.label_ids.shape[0], p.label_ids.shape[1] + 1), dtype=np.int32)
+        y_pred[:, :-1] = preds
+        y_pred[:, -1] = (np.sum(preds, axis=1) == 0).astype('int32')
+        # Compute scores
+        macro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0)
+        micro_f1 = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)
+        return {'macro-f1': macro_f1, 'micro-f1': micro_f1}
+
+    # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.
+    if data_args.pad_to_max_length:
+        data_collator = default_data_collator
+    elif training_args.fp16:
+        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)
+    else:
+        data_collator = None
+
+    # Initialize our Trainer
+    trainer = MultilabelTrainer(
+        model=model,
+        args=training_args,
+        train_dataset=train_dataset if training_args.do_train else None,
+        eval_dataset=eval_dataset if training_args.do_eval else None,
+        compute_metrics=compute_metrics,
+        tokenizer=tokenizer,
+        data_collator=data_collator,
+        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
+    )
+
+    # Training
+    if training_args.do_train:
+        checkpoint = None
+        if training_args.resume_from_checkpoint is not None:
+            checkpoint = training_args.resume_from_checkpoint
+        elif last_checkpoint is not None:
+            checkpoint = last_checkpoint
+        train_result = trainer.train(resume_from_checkpoint=checkpoint)
+        metrics = train_result.metrics
+        max_train_samples = (
+            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)
+        )
+        metrics["train_samples"] = min(max_train_samples, len(train_dataset))
+
+        trainer.save_model()  # Saves the tokenizer too for easy upload
+
+        trainer.log_metrics("train", metrics)
+        trainer.save_metrics("train", metrics)
+        trainer.save_state()
+
+    # Evaluation
+    if training_args.do_eval:
+        logger.info("*** Evaluate ***")
+        metrics = trainer.evaluate(eval_dataset=eval_dataset)
+
+        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)
+        metrics["eval_samples"] = min(max_eval_samples, len(eval_dataset))
+
+        trainer.log_metrics("eval", metrics)
+        trainer.save_metrics("eval", metrics)
+
+    # Prediction
+    if training_args.do_predict:
+        logger.info("*** Predict ***")
+        predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix="predict")
+
+        max_predict_samples = (
+            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)
+        )
+        metrics["predict_samples"] = min(max_predict_samples, len(predict_dataset))
+
+        trainer.log_metrics("predict", metrics)
+        trainer.save_metrics("predict", metrics)
+
+        output_predict_file = os.path.join(training_args.output_dir, "test_predictions.csv")
+        if trainer.is_world_process_zero():
+            with open(output_predict_file, "w") as writer:
+                for index, pred_list in enumerate(predictions[0]):
+                    pred_line = '\t'.join([f'{pred:.5f}' for pred in pred_list])
+                    writer.write(f"{index}\t{pred_line}\n")
+
+    # Clean up checkpoints
+    checkpoints = [filepath for filepath in glob.glob(f'{training_args.output_dir}/*/') if '/checkpoint' in filepath]
+    for checkpoint in checkpoints:
+        shutil.rmtree(checkpoint)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/experiments/eurlex.py b/experiments/eurlex.py
index 8acf0dd..75b1d97 100644
--- a/experiments/eurlex.py
+++ b/experiments/eurlex.py
@@ -5,15 +5,18 @@
 import logging
 import os
 import random
+
 import sys
 from dataclasses import dataclass, field
 from typing import Optional
 
 import datasets
+
 from datasets import load_dataset
 from sklearn.metrics import f1_score
 from trainer import MultilabelTrainer
 from scipy.special import expit
+
 import glob
 import shutil
 
@@ -30,10 +33,16 @@ from transformers import (
     set_seed,
     EarlyStoppingCallback,
 )
+
 from transformers.trainer_utils import get_last_checkpoint
 from transformers.utils import check_min_version
 from transformers.utils.versions import require_version
-
+try:
+    import idr_torch
+    JZ = True
+except ModuleNotFoundError:
+    JZ = False
+import torch.distributed as dist
 
 # Will error if the minimal version of Transformers is not installed. Remove at your own risks.
 check_min_version("4.9.0")
@@ -91,6 +100,12 @@ class DataTrainingArguments:
             "value if set."
         },
     )
+    task: Optional[str] = field(
+        default='ecthr_a',
+        metadata={
+            "help": "Define downstream task"
+        },
+    )
     server_ip: Optional[str] = field(default=None, metadata={"help": "For distant debugging."})
     server_port: Optional[str] = field(default=None, metadata={"help": "For distant debugging."})
 
@@ -140,9 +155,20 @@ def main():
     # or by passing the --help flag to this script.
     # We now keep distinct sets of args, for a cleaner separation of concerns.
 
+    if JZ:
+        dist.init_process_group(
+            backend='nccl',
+            init_method='env://',
+            world_size=idr_torch.size,
+            rank=idr_torch.rank
+        )
+
     parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
     model_args, data_args, training_args = parser.parse_args_into_dataclasses()
 
+    if JZ:
+        training_args.local_rank = idr_torch.local_rank
+
     # Setup distant debugging if needed
     if data_args.server_ip and data_args.server_port:
         # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script
@@ -202,13 +228,13 @@ def main():
     # download the dataset.
     # Downloading and loading eurlex dataset from the hub.
     if training_args.do_train:
-        train_dataset = load_dataset("lex_glue", "eurlex", split="train", cache_dir=model_args.cache_dir)
+        train_dataset = datasets.load_from_disk(f"{model_args.cache_dir}/{data_args.task}/train")
 
     if training_args.do_eval:
-        eval_dataset = load_dataset("lex_glue", "eurlex", split="validation", cache_dir=model_args.cache_dir)
+        eval_dataset = datasets.load_from_disk(f"{model_args.cache_dir}/{data_args.task}/validation")
 
     if training_args.do_predict:
-        predict_dataset = load_dataset("lex_glue", "eurlex", split="test", cache_dir=model_args.cache_dir)
+        predict_dataset = datasets.load_from_disk(f"{model_args.cache_dir}/{data_args.task}/test")
 
     # Labels
     label_list = list(range(100))
diff --git a/experiments/ledgar.py b/experiments/ledgar.py
index 7f3a98f..b35266f 100644
--- a/experiments/ledgar.py
+++ b/experiments/ledgar.py
@@ -5,14 +5,18 @@
 import logging
 import os
 import random
+
 import sys
 from dataclasses import dataclass, field
 from typing import Optional
 
 import datasets
+import numpy as np
 from datasets import load_dataset
 from sklearn.metrics import f1_score
-import numpy as np
+
+
+
 import glob
 import shutil
 
@@ -33,7 +37,12 @@ from transformers import (
 from transformers.trainer_utils import get_last_checkpoint
 from transformers.utils import check_min_version
 from transformers.utils.versions import require_version
-
+try:
+    import idr_torch
+    JZ = True
+except ModuleNotFoundError:
+    JZ = False
+import torch.distributed as dist
 
 # Will error if the minimal version of Transformers is not installed. Remove at your own risks.
 check_min_version("4.9.0")
@@ -91,6 +100,12 @@ class DataTrainingArguments:
             "value if set."
         },
     )
+    task: Optional[str] = field(
+        default='ecthr_a',
+        metadata={
+            "help": "Define downstream task"
+        },
+    )
     server_ip: Optional[str] = field(default=None, metadata={"help": "For distant debugging."})
     server_port: Optional[str] = field(default=None, metadata={"help": "For distant debugging."})
 
@@ -140,9 +155,20 @@ def main():
     # or by passing the --help flag to this script.
     # We now keep distinct sets of args, for a cleaner separation of concerns.
 
+    if JZ:
+        dist.init_process_group(
+            backend='nccl',
+            init_method='env://',
+            world_size=idr_torch.size,
+            rank=idr_torch.rank
+        )
+
     parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
     model_args, data_args, training_args = parser.parse_args_into_dataclasses()
 
+    if JZ:
+        training_args.local_rank = idr_torch.local_rank
+
     # Setup distant debugging if needed
     if data_args.server_ip and data_args.server_port:
         # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script
@@ -202,13 +228,13 @@ def main():
     # download the dataset.
     # Downloading and loading eurlex dataset from the hub.
     if training_args.do_train:
-        train_dataset = load_dataset("lex_glue", "ledgar", split="train", cache_dir=model_args.cache_dir)
+        train_dataset = datasets.load_from_disk(f"{model_args.cache_dir}/{data_args.task}/train")
 
     if training_args.do_eval:
-        eval_dataset = load_dataset("lex_glue", "ledgar", split="validation", cache_dir=model_args.cache_dir)
+        eval_dataset = datasets.load_from_disk(f"{model_args.cache_dir}/{data_args.task}/validation")
 
     if training_args.do_predict:
-        predict_dataset = load_dataset("lex_glue", "ledgar", split="test", cache_dir=model_args.cache_dir)
+        predict_dataset = datasets.load_from_disk(f"{model_args.cache_dir}/{data_args.task}/test")
 
     # Labels
     label_list = list(range(100))
diff --git a/experiments/scotus.py b/experiments/scotus.py
index e37662d..5afdc66 100644
--- a/experiments/scotus.py
+++ b/experiments/scotus.py
@@ -11,17 +11,17 @@ from dataclasses import dataclass, field
 from typing import Optional
 
 import datasets
+import numpy as np
 from datasets import load_dataset
 from sklearn.metrics import f1_score
-from models.hierbert import HierarchicalBert
-import numpy as np
+
+#from models.hierbert import HierarchicalBert
 from torch import nn
 import glob
 import shutil
 
 import transformers
 from transformers import (
-    Trainer,
     AutoConfig,
     AutoModelForSequenceClassification,
     AutoTokenizer,
@@ -32,12 +32,17 @@ from transformers import (
     default_data_collator,
     set_seed,
     EarlyStoppingCallback,
+    Trainer
 )
 from transformers.trainer_utils import get_last_checkpoint
 from transformers.utils import check_min_version
 from transformers.utils.versions import require_version
-from models.deberta import DebertaForSequenceClassification
-
+try:
+    import idr_torch
+    JZ = True
+except ModuleNotFoundError:
+    JZ = False
+import torch.distributed as dist
 
 # Will error if the minimal version of Transformers is not installed. Remove at your own risks.
 check_min_version("4.9.0")
@@ -109,6 +114,12 @@ class DataTrainingArguments:
             "value if set."
         },
     )
+    task: Optional[str] = field(
+        default='ecthr_a',
+        metadata={
+            "help": "Define downstream task"
+        },
+    )
     server_ip: Optional[str] = field(default=None, metadata={"help": "For distant debugging."})
     server_port: Optional[str] = field(default=None, metadata={"help": "For distant debugging."})
 
@@ -161,9 +172,20 @@ def main():
     # or by passing the --help flag to this script.
     # We now keep distinct sets of args, for a cleaner separation of concerns.
 
+    if JZ:
+        dist.init_process_group(
+            backend='nccl',
+            init_method='env://',
+            world_size=idr_torch.size,
+            rank=idr_torch.rank
+        )
+
     parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
     model_args, data_args, training_args = parser.parse_args_into_dataclasses()
 
+    if JZ:
+        training_args.local_rank = idr_torch.local_rank
+
     # Setup distant debugging if needed
     if data_args.server_ip and data_args.server_port:
         # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script
@@ -227,13 +249,15 @@ def main():
     # download the dataset.
     # Downloading and loading eurlex dataset from the hub.
     if training_args.do_train:
-        train_dataset = load_dataset("lex_glue", "scotus", split="train", cache_dir=model_args.cache_dir)
+        train_dataset = datasets.load_from_disk(f"{model_args.cache_dir}/{data_args.task}/train")
+        #print(train_dataset)
+        #print(data_args.task)
 
     if training_args.do_eval:
-        eval_dataset = load_dataset("lex_glue", "scotus", split="validation", cache_dir=model_args.cache_dir)
+        eval_dataset = datasets.load_from_disk(f"{model_args.cache_dir}/{data_args.task}/validation")
 
     if training_args.do_predict:
-        predict_dataset = load_dataset("lex_glue", "scotus", split="test", cache_dir=model_args.cache_dir)
+        predict_dataset = datasets.load_from_disk(f"{model_args.cache_dir}/{data_args.task}/test")
 
     # Labels
     label_list = list(range(14))
@@ -245,7 +269,7 @@ def main():
     config = AutoConfig.from_pretrained(
         model_args.config_name if model_args.config_name else model_args.model_name_or_path,
         num_labels=num_labels,
-        finetuning_task="scotus",
+        finetuning_task=f"{data_args.task}",
         cache_dir=model_args.cache_dir,
         revision=model_args.model_revision,
         use_auth_token=True if model_args.use_auth_token else None,
diff --git a/experiments/unfair_tos.py b/experiments/unfair_tos.py
index c1ddfdb..f5d6c75 100644
--- a/experiments/unfair_tos.py
+++ b/experiments/unfair_tos.py
@@ -5,18 +5,20 @@
 import logging
 import os
 import random
+
 import sys
 from dataclasses import dataclass, field
 from typing import Optional
 
 import datasets
+import numpy as np
 from datasets import load_dataset
 from sklearn.metrics import f1_score
 from trainer import MultilabelTrainer
 from scipy.special import expit
+import torch
 import glob
 import shutil
-import numpy as np
 
 import transformers
 from transformers import (
@@ -31,10 +33,16 @@ from transformers import (
     set_seed,
     EarlyStoppingCallback,
 )
+
 from transformers.trainer_utils import get_last_checkpoint
 from transformers.utils import check_min_version
 from transformers.utils.versions import require_version
-
+try:
+    import idr_torch
+    JZ = True
+except ModuleNotFoundError:
+    JZ = False
+import torch.distributed as dist
 
 # Will error if the minimal version of Transformers is not installed. Remove at your own risks.
 check_min_version("4.9.0")
@@ -92,6 +100,12 @@ class DataTrainingArguments:
             "value if set."
         },
     )
+    task: Optional[str] = field(
+        default='ecthr_a',
+        metadata={
+            "help": "Define downstream task"
+        },
+    )
     server_ip: Optional[str] = field(default=None, metadata={"help": "For distant debugging."})
     server_port: Optional[str] = field(default=None, metadata={"help": "For distant debugging."})
 
@@ -141,9 +155,20 @@ def main():
     # or by passing the --help flag to this script.
     # We now keep distinct sets of args, for a cleaner separation of concerns.
 
+    if JZ:
+        dist.init_process_group(
+            backend='nccl',
+            init_method='env://',
+            world_size=idr_torch.size,
+            rank=idr_torch.rank
+        )
+
     parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
     model_args, data_args, training_args = parser.parse_args_into_dataclasses()
 
+    if JZ:
+        training_args.local_rank = idr_torch.local_rank
+
     # Setup distant debugging if needed
     if data_args.server_ip and data_args.server_port:
         # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script
@@ -203,13 +228,19 @@ def main():
     # download the dataset.
     # Downloading and loading eurlex dataset from the hub.
     if training_args.do_train:
-        train_dataset = load_dataset("lex_glue", "unfair_tos", split="train", data_dir='data', cache_dir=model_args.cache_dir)
+        train_dataset = datasets.load_from_disk(f"{model_args.cache_dir}/{data_args.task}/train")
+        print(f"Training dataset loaded with size: {len(train_dataset)}")
+        print("Example from training dataset:", train_dataset[0])
 
     if training_args.do_eval:
-        eval_dataset = load_dataset("lex_glue", "unfair_tos", split="validation", data_dir='data', cache_dir=model_args.cache_dir)
+        eval_dataset = datasets.load_from_disk(f"{model_args.cache_dir}/{data_args.task}/validation")
+        print(f"Validation dataset loaded with size: {len(eval_dataset)}")
+        print("Example from validation dataset:", eval_dataset[0])
 
     if training_args.do_predict:
-        predict_dataset = load_dataset("lex_glue", "unfair_tos", split="test", data_dir='data', cache_dir=model_args.cache_dir)
+        predict_dataset = datasets.load_from_disk(f"{model_args.cache_dir}/{data_args.task}/test")
+        print(f"Test dataset loaded with size: {len(predict_dataset)}")
+        print("Example from test dataset:", predict_dataset[0])
 
     # Labels
     label_list = list(range(8))
-- 
2.42.0

