# -*- coding: utf-8 -*-
"""07-12-23-Training-Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C0WxSOIbEkqfc2i8h_vIXkiQbiszvUlb

## Import Library
"""


import pandas as pd
import numpy as np
import collections
import math

import os

from transformers import AutoModelForMaskedLM, AutoTokenizer, TrainingArguments, Trainer
from datasets import Dataset, DatasetDict, load_dataset
from transformers import DataCollatorForWholeWordMask, default_data_collator
import torch
import pandas as pd
from datasets import Dataset

import nltk
nltk.download('stopwords')



"""## Load Data

TODO:


*   See how to lead each dataset
*   How to merge it
*   How to shuffle it into train/test
*   Batch optimize within each type of data (search on the impact of the batch during training time)
*   Display data

"""

# #============== Method 1 =========================
# # Assuming dataset is already loaded
# # Dictionary to hold sub-corpus datasets
# sub_corpus_datasets = {}

# # Iterate over all configurations (sub-corpora) in the dataset
# for sub_corpus in dataset.keys():
#     sub_corpus_datasets[sub_corpus] = dataset[sub_corpus]

# # Now, sub_corpus_datasets dictionary will have each sub-corpus as a separate dataset

# #============== Method 2 =========================
# # List of sub-corpus aliases in the LeXFiles dataset
# sub_corpora = ['eu-legislation', 'eu-court-cases', 'ecthr-cases', 'uk-legislation',
#                'uk-court-cases', 'indian-court-cases', 'canadian-legislation',
#                'canadian-court-cases', 'us-court-cases', 'us-legislation', 'us-contracts']

# # Load each sub-corpus and store in a dictionary
# datasets = {corpus: load_dataset('lexlms/lex_files', name=corpus) for corpus in sub_corpora}

# Commented out IPython magic to ensure Python compatibility.
#drive_path = '/content/drive/MyDrive/LEXHNOLOGY/Data/Data/LeXFiles'

# # Navigate to the specified directory
# %cd {drive_path}

import os
import json
from datasets import Dataset, load_dataset

# Define the directory where your JSON files are stored
data_dir = './data'


# Function to load a dataset from a JSON file
def load_dataset_from_json(file_path):
    with open(file_path, 'r') as file:
        # Read the file into a list of dictionaries
        data_list = [json.loads(line) for line in file]

    # Transform list of dictionaries to dictionary of lists
    data = {key: [dic[key] for dic in data_list] for key in data_list[0]}

    return Dataset.from_dict(data)

# Load each dataset
dataset_dict = {}
for split in ['train', 'validation', 'test']:
    file_path = os.path.join(data_dir, f'{split}.json')
    if os.path.exists(file_path):
        dataset_dict[split] = load_dataset_from_json(file_path)

# dataset_dict is now a dictionary of Dataset objects


# Iterate through each dataset
for data_type, dataset in dataset_dict.items():
    print(f"Examples from {data_type} dataset:")

    # Print first few examples
    for i in range(5):  # Adjust the number to print more or fewer examples
        print(dataset[i])
    print("\n")

total_size_bytes = sum(ds._data.nbytes for ds in dataset_dict.values())
total_size_gb = total_size_bytes / 1e9  # Convert bytes to gigabytes

print(f"Total size of datasets: {total_size_gb:.3f} GB")

"""### Take a small data"""

# Number of examples you want to select from each split
num_examples = 100  # You can adjust this number based on your testing needs

# Create a smaller dataset for testing
small_datasets = {split: dataset.select(range(num_examples)) for split, dataset in dataset_dict.items()}

# Now, small_datasets contains a small subset of each original dataset split


"""## Load Model
TODO:
*  Load the model bert base uncased because it used on the legal BERT.
*  Verify the parameter
*  Test masking on some example from the dataset.
"""

model_checkpoint = "models/bert-base-uncased"
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)

device = torch.device('cuda' if torch.cuda.is_available() else "cpu")
print("====================================================================")
print(device)
model.to(device)

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

bert_num_parameters = model.num_parameters() / 1_000_000
print(f"'>>> BERT number of parameters: {round(bert_num_parameters)}M'")
print(f"'>>> BERT number of parameters: 110M'")
print("====================================================================")

# Text Masking on an example

text = "This appeal coming on for hearing this [MASK]."
inputs = tokenizer(text, return_tensors="pt")
inputs.to(device)
token_logits = model(**inputs).logits
# Find the location of [MASK] and extract its logits
mask_token_index = torch.where(inputs["input_ids"] == tokenizer.mask_token_id)[1]
mask_token_logits = token_logits[0, mask_token_index, :]
# Pick the [MASK] candidates with the highest logits
top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()

for token in top_5_tokens:
    print(f"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'")

print("====================================================================")


"""## Tokenization
* Working on each data independanttly or concatenate and after that I split
* Merging document may lead to confuse between the different legal system. (solution: split before merging)
* There is no impact of whole word concept, so i can use it.
* Affichage.

"""

# Use fast tokenizer to do the whole word masking
def tokenize_function(examples):
    result = tokenizer(examples["text"])
    if tokenizer.is_fast:
        result["word_ids"] = [result.word_ids(i) for i in range(len(result["input_ids"]))]
    return result


# Apply tokenization to each dataset in the dictionary
tokenized_datasets = {}
for split, dataset in small_datasets.items():
    tokenized_datasets[split] = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=['id', 'text', 'sector', 'descriptor', 'year', '__index_level_0__']
    )

# # Concatener toutes les phrases et aprÃ¨s on segmente
# chunk_size = 128
# def group_texts(examples):
#     # Concatenate all texts
#     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
#     # Compute length of concatenated texts
#     total_length = len(concatenated_examples[list(examples.keys())[0]])
#     # We drop the last chunk if it's smaller than chunk_size
#     total_length = (total_length // chunk_size) * chunk_size
#     # Split by chunks of max_len
#     result = {
#         k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]
#         for k, t in concatenated_examples.items()
#     }
#     # Create a new labels column
#     result["labels"] = result["input_ids"].copy()
#     return result

chunk_size = 128

def group_texts(examples):
    # Process each example (document) individually
    result = {key: [] for key in examples.keys()}
    for index in range(len(examples["input_ids"])):
        # Extract single document
        single_doc = {key: examples[key][index] for key in examples.keys()}
        # Compute length
        doc_length = len(single_doc["input_ids"])
        # Adjust length to fit the chunk size
        doc_length = (doc_length // chunk_size) * chunk_size
        # Segment into chunks
        for i in range(0, doc_length, chunk_size):
            chunk = {key: single_doc[key][i:i + chunk_size] for key in single_doc.keys()}
            # Add chunk to result
            for key in chunk.keys():
                result[key].append(chunk[key])
    # Duplicate input_ids to labels if needed
    result["labels"] = result["input_ids"].copy()
    return result

# Apply the 'group_texts' function to each dataset in the dictionary
for split in tokenized_datasets.keys():
    tokenized_datasets[split] = tokenized_datasets[split].map(group_texts, batched=True)

print("====================================================================")

print(tokenizer.decode(tokenized_datasets["train"][0]["input_ids"]))

print(tokenizer.decode(tokenized_datasets["train"][0]["labels"]))

"""## Data Collator

* I think in the first step, I will use the same predifined one. (because is just a baseline model without any modification on masking words)
* See if I keep the same function of whole word or I will use the predifined (Set the advantage and read the documentation).
* I don't know if i will modify on the predifined class.
"""

data_collator = DataCollatorForWholeWordMask(
    tokenizer=tokenizer,
    return_tensors="pt",
)

# Example to see how Data Collator work
samples = [tokenized_datasets["train"][i] for i in range(2)]
for sample in samples:
    _ = sample.pop("word_ids")
print("====================================================================")

for chunk in data_collator(samples)["input_ids"]:
    print(f"\n'>>> {tokenizer.decode(chunk)}'")
print("====================================================================")

"""## Training Stage

TODO:
* Try with a lower number of epochs and steps to see how take the experiment in term of time and units.
* How i can save the models.
* Split data
* How to evaluate the quality of masking.
"""

# Experiment with less data
train_size = 100
test_size = int(0.1 * train_size)

downsampled_dataset = tokenized_datasets["train"].train_test_split(
    train_size=train_size, test_size=test_size, seed=42
)
downsampled_dataset

batch_size = 8
# Show the training loss with every epoch
logging_steps = len(tokenized_datasets["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

output_dir = f"models/{model_name}-baseline"

training_args = TrainingArguments(
    output_dir=output_dir,  # Modified to save in Google Drive
    #overwrite_output_dir=True,
    evaluation_strategy="epoch",
        num_train_epochs= 3,
    learning_rate=2e-5,
    weight_decay=0.01,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    push_to_hub=False,  # Set to False unless you also want to push to Hugging Face's Model Hub
    # fp16=True, # when we use cuda
    logging_steps=logging_steps,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator, #=====
    tokenizer=tokenizer,
)

"""### Train"""

import math

eval_results = trainer.evaluate()
print(f">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}")

trainer.train()

eval_results = trainer.evaluate()
print(f">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}")

import math

# Evaluate the model on the test set
test_results = trainer.evaluate(eval_dataset=tokenized_datasets['test'])

# Calculate and print the perplexity, if applicable
# Note: Perplexity is relevant for language models, and it's calculated as exp(loss)
if 'eval_loss' in test_results:
    print(f">>> Test Perplexity: {math.exp(test_results['eval_loss']):.2f}")
else:
    print("Evaluation loss ('eval_loss') not found in test_results")

"""## Save Models
* Search the possibility of saving model on the hugging face warebouse.
* How many allocated GO for saving the models...
"""

